{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823d9f1b",
   "metadata": {},
   "source": [
    "# Sentiment140 Text Processing Pipeline\n",
    "Dataset: [Sentiment140 (Kaggle)](https://www.kaggle.com/datasets/kazanova/sentiment140).\n",
    "Each section below corresponds to one assignment task.\n",
    "\n",
    "## 1. Text Cleaning\n",
    "Lowercase tweets, expand contractions, and remove emojis or special symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafb6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "DATA_PATH = \"data/data1.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(DATA_PATH, encoding=\"ISO-8859-1\")\n",
    "\n",
    "if list(df.columns) != [\"sentiment\", \"tweet_id\", \"timestamp\", \"query\", \"user\", \"text\"]:\n",
    "    df.columns = [\"sentiment\", \"tweet_id\", \"timestamp\", \"query\", \"user\", \"text\"]\n",
    "\n",
    "CONTRACTIONS = {\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractions_pattern = re.compile(\n",
    "    \"(\" + \"|\".join(map(re.escape, CONTRACTIONS.keys())) + \")\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\",\n",
    "    flags=re.UNICODE,\n",
    ")\n",
    "special_pattern = re.compile(r\"[^a-z0-9\\s]\")\n",
    "\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    return contractions_pattern.sub(lambda match: CONTRACTIONS[match.group(0).lower()], text)\n",
    "\n",
    "\n",
    "def clean_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if pd.isna(text) else str(text)\n",
    "    lowered = text.lower()\n",
    "    expanded = expand_contractions(lowered)\n",
    "    no_emoji = emoji_pattern.sub(\" \", expanded)\n",
    "    cleaned = special_pattern.sub(\" \", no_emoji)\n",
    "    return re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df[[\"text\", \"clean_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d78f6e",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "Split each cleaned tweet into alphanumeric tokens to prepare for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"\\b[a-z0-9]+\\b\")\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if pd.isna(text) else str(text)\n",
    "    return token_pattern.findall(text.lower())\n",
    "\n",
    "\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize)\n",
    "print(\"Tokenized rows:\", len(df[\"tokens\"]))\n",
    "df[[\"clean_text\", \"tokens\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68804d2d",
   "metadata": {},
   "source": [
    "## 3. Stop Words Removal\n",
    "Filter common function words to focus on content-bearing terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d476ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "df[\"tokens_no_stop\"] = df[\"tokens\"].apply(lambda toks: [t for t in toks if t not in STOP_WORDS])\n",
    "df[[\"clean_text\", \"tokens_no_stop\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e0f98",
   "metadata": {},
   "source": [
    "## 4. Stemming vs. Lemmatization\n",
    "Build a vocabulary to compare how Porter stemming and WordNet lemmatization transform words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06216f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "\n",
    "unique_words = sorted(set(chain.from_iterable(df[\"tokens_no_stop\"])))\n",
    "print(f\"Unique vocabulary size: {len(unique_words):,}\")\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "vocab_df = pd.DataFrame({\n",
    "    \"word\": unique_words,\n",
    "})\n",
    "vocab_df[\"stem\"] = vocab_df[\"word\"].apply(porter.stem)\n",
    "vocab_df[\"lemma\"] = vocab_df[\"word\"].apply(lemmatizer.lemmatize)\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "total_words = len(vocab_df)\n",
    "stem_changed = (vocab_df[\"stem\"] != vocab_df[\"word\"]).sum()\n",
    "lemma_changed = (vocab_df[\"lemma\"] != vocab_df[\"word\"]).sum()\n",
    "stem_vs_lemma_diff = (vocab_df[\"stem\"] != vocab_df[\"lemma\"]).sum()\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\"metric\": \"Total unique words\", \"count\": total_words},\n",
    "        {\"metric\": \"Words altered by stemming\", \"count\": stem_changed},\n",
    "        {\"metric\": \"Words altered by lemmatization\", \"count\": lemma_changed},\n",
    "        {\"metric\": \"Stem vs lemma disagree\", \"count\": stem_vs_lemma_diff},\n",
    "    ]\n",
    ")\n",
    "display(summary_df)\n",
    "\n",
    "diff_examples = vocab_df[vocab_df[\"stem\"] != vocab_df[\"lemma\"]][[\"word\", \"stem\", \"lemma\"]].head(10)\n",
    "display(diff_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca3ff5",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction (BoW & TF-IDF)\n",
    "Vectorize the cleaned tweets with capped vocabulary to create sparse Bag-of-Words and TF-IDF representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dabae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "MAX_FEATURES = 5000  # cap vocabulary to keep matrix size manageable\n",
    "MIN_DF = 5\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=MAX_FEATURES, min_df=MIN_DF)\n",
    "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(bow_matrix)\n",
    "\n",
    "print(\"BoW matrix shape:\", bow_matrix.shape)\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"Vocabulary size:\", len(count_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a212af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_sample = pd.DataFrame(\n",
    "    bow_matrix[:5].toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out(),\n",
    "    dtype=int,\n",
    ")\n",
    "tfidf_sample = pd.DataFrame(\n",
    "    tfidf_matrix[:5].toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out(),\n",
    ")\n",
    "\n",
    "display(bow_sample.head())\n",
    "display(tfidf_sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e8a51",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "Highlight the most common terms using a word cloud and bar chart to interpret the cleaned corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "token_counts = Counter(chain.from_iterable(df[\"tokens_no_stop\"]))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(token_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Filtered Tokens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b32cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = token_counts.most_common(10)\n",
    "top_df = pd.DataFrame(top_words, columns=[\"word\", \"count\"])\n",
    "display(top_df)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_df[\"word\"], top_df[\"count\"], color=\"steelblue\")\n",
    "plt.title(\"Top 10 Words by Frequency\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc02b2",
   "metadata": {},
   "source": [
    "## 7. N-grams\n",
    "Extract consecutive word pairs (bigrams) from stop-word stripped tokens to surface short phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c545fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def extract_bigrams(tokens):\n",
    "    if len(tokens) < 2:\n",
    "        return []\n",
    "    return list(zip(tokens[:-1], tokens[1:]))\n",
    "\n",
    "bigram_counter = Counter()\n",
    "for toks in df[\"tokens_no_stop\"]:\n",
    "    bigram_counter.update(extract_bigrams(toks))\n",
    "\n",
    "top_bigrams = bigram_counter.most_common(5)\n",
    "top_bigrams_df = pd.DataFrame(top_bigrams, columns=[\"bigram\", \"count\"])\n",
    "top_bigrams_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
